{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Network\n",
    "\n",
    "In this notebook, we will train the CNN-RNN model for Image captioning\n",
    "\n",
    "CNN [ResNet](https://arxiv.org/pdf/1512.03385.pdf) model is used for feature extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\pradeep\n",
      "[nltk_data]     dubey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "from data_loader import get_loader\n",
    "from data_loader_val import get_loader as val_get_loader\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from nlp_utils import clean_sentence, bleu_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['annotations', 'images']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset dir path\n",
    "cocoapi_dir = r\"../cocoapi/\"\n",
    "\n",
    "import os\n",
    "folders = [folder for folder in os.listdir(\"../cocoapi/\")]\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # batch size\n",
    "vocab_threshold = 5  # minimum word count threshold\n",
    "vocab_from_file = True  # if True, load existing vocab file\n",
    "embed_size = 256  # dimensionality of image and word embeddings\n",
    "hidden_size = 512  # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 2  # number of training epochs\n",
    "save_every = 1  # determines frequency of saving model weights\n",
    "print_every = 20  # determines window for printing average loss\n",
    "log_file = \"training_log.txt\"  # name of file with saved training loss and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        # smaller edge of image resized to 256\n",
    "        transforms.Resize(256),\n",
    "        # get 224x224 crop from random location\n",
    "        transforms.RandomCrop(224),\n",
    "        # horizontally flip image with probability=0.5\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # convert the PIL Image to a tensor\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "            (0.229, 0.224, 0.225),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\pradeep\n",
      "[nltk_data]     dubey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\pradeep\n",
      "[nltk_data]     dubey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('stopwords')  # For stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\pradeep\n",
      "[nltk_data]     dubey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\anaconda3\\envs\\myvenv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\anaconda3\\envs\\myvenv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\anaconda3\\envs\\myvenv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\anaconda3\\envs\\myvenv\\lib\\site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda3\\envs\\myvenv\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\envs\\myvenv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "!pip install --upgrade nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.72s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 591753/591753 [00:33<00:00, 17487.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build data loader.\n",
    "data_loader = get_loader(\n",
    "    transform=transform_train,\n",
    "    mode=\"train\",\n",
    "    batch_size=batch_size,\n",
    "    vocab_threshold=vocab_threshold,\n",
    "    vocab_from_file=vocab_from_file,\n",
    "    cocoapi_loc=cocoapi_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Encoder and RNN Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# ----------- Encoder ------------\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # disable learning for parameters\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "# --------- Decoder ----------\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_size: final embedding size of the CNN encoder\n",
    "            hidden_size: hidden size of the LSTM\n",
    "            vocab_size: size of the vocabulary\n",
    "            num_layers: number of layers of the LSTM\n",
    "        \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Assigning hidden dimension\n",
    "        self.hidden_dim = hidden_size\n",
    "        # Map each word index to a dense word embedding tensor of embed_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        # Creating LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Initializing linear to apply at last of RNN layer for further prediction\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        # Initializing values for hidden and cell state\n",
    "        self.hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: features tensor. shape is (bs, embed_size)\n",
    "            captions: captions tensor. shape is (bs, cap_length)\n",
    "        Returns:\n",
    "            outputs: scores of the linear layer\n",
    "\n",
    "        \"\"\"\n",
    "        # remove <end> token from captions and embed captions\n",
    "        cap_embedding = self.embed(\n",
    "            captions[:, :-1]\n",
    "        )  # (bs, cap_length) -> (bs, cap_length-1, embed_size)\n",
    "\n",
    "        embeddings = torch.cat((features.unsqueeze(dim=1), cap_embedding), dim=1)\n",
    "\n",
    "        #  getting output i.e. score and hidden layer.\n",
    "        # first value: all the hidden states throughout the sequence. second value: the most recent hidden state\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeddings\n",
    "        )  # (bs, cap_length, hidden_size), (1, bs, hidden_size)\n",
    "        outputs = self.linear(lstm_out)  # (bs, cap_length, vocab_size)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, inputs, states=None, max_len=20):\n",
    "        \"\"\"\n",
    "        accepts pre-processed image tensor (inputs) and returns predicted\n",
    "        sentence (list of tensor ids of length max_len)\n",
    "        Args:\n",
    "            inputs: shape is (1, 1, embed_size)\n",
    "            states: initial hidden state of the LSTM\n",
    "            max_len: maximum length of the predicted sentence\n",
    "\n",
    "        Returns:\n",
    "            res: list of predicted words indices\n",
    "        \"\"\"\n",
    "        res = []\n",
    "\n",
    "        # Now we feed the LSTM output and hidden states back into itself to get the caption\n",
    "        for i in range(max_len):\n",
    "            lstm_out, states = self.lstm(\n",
    "                inputs, states\n",
    "            )  # lstm_out: (1, 1, hidden_size)\n",
    "            outputs = self.linear(lstm_out.squeeze(dim=1))  # outputs: (1, vocab_size)\n",
    "            _, predicted_idx = outputs.max(dim=1)  # predicted: (1, 1)\n",
    "            res.append(predicted_idx.item())\n",
    "            # if the predicted idx is the stop index, the loop stops\n",
    "            if predicted_idx == 1:\n",
    "                break\n",
    "            inputs = self.embed(predicted_idx)  # inputs: (1, embed_size)\n",
    "            # prepare input for next iteration\n",
    "            inputs = inputs.unsqueeze(1)  # inputs: (1, 1, embed_size)\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is :  11543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "print(\"vocab size is : \",vocab_size)\n",
    "\n",
    "# Initializing the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Defining the loss function\n",
    "criterion = (\n",
    "    nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    ")\n",
    "\n",
    "# Specifying the learnable parameters of the mode\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Defining the optimize\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoc\n",
    "total_step = math.ceil(len(data_loader.dataset) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9247\n"
     ]
    }
   ],
   "source": [
    "print(total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [20/9247], Loss: 4.8898, Perplexity: 132.9242\n",
      "Epoch [1/2], Step [40/9247], Loss: 4.4430, Perplexity: 85.0295\n",
      "Epoch [1/2], Step [60/9247], Loss: 4.0426, Perplexity: 56.9726\n",
      "Epoch [1/2], Step [80/9247], Loss: 3.9417, Perplexity: 51.5075\n",
      "Epoch [1/2], Step [100/9247], Loss: 3.8917, Perplexity: 48.9926\n",
      "Epoch [1/2], Step [120/9247], Loss: 4.0058, Perplexity: 54.9168\n",
      "Epoch [1/2], Step [140/9247], Loss: 4.0706, Perplexity: 58.5938\n",
      "Epoch [1/2], Step [160/9247], Loss: 3.6049, Perplexity: 36.7782\n",
      "Epoch [1/2], Step [180/9247], Loss: 3.6711, Perplexity: 39.2963\n",
      "Epoch [1/2], Step [200/9247], Loss: 3.9004, Perplexity: 49.4227\n",
      "Epoch [1/2], Step [220/9247], Loss: 3.9320, Perplexity: 51.0091\n",
      "Epoch [1/2], Step [240/9247], Loss: 3.4457, Perplexity: 31.3646\n",
      "Epoch [1/2], Step [260/9247], Loss: 3.2511, Perplexity: 25.8196\n",
      "Epoch [1/2], Step [280/9247], Loss: 3.2402, Perplexity: 25.5392\n",
      "Epoch [1/2], Step [300/9247], Loss: 3.3727, Perplexity: 29.1566\n",
      "Epoch [1/2], Step [320/9247], Loss: 3.4637, Perplexity: 31.9363\n",
      "Epoch [1/2], Step [340/9247], Loss: 3.2710, Perplexity: 26.3389\n",
      "Epoch [1/2], Step [360/9247], Loss: 3.3126, Perplexity: 27.4573\n",
      "Epoch [1/2], Step [380/9247], Loss: 3.2273, Perplexity: 25.2122\n",
      "Epoch [1/2], Step [400/9247], Loss: 3.5221, Perplexity: 33.8559\n",
      "Epoch [1/2], Step [420/9247], Loss: 3.5558, Perplexity: 35.0156\n",
      "Epoch [1/2], Step [440/9247], Loss: 3.5932, Perplexity: 36.3488\n",
      "Epoch [1/2], Step [460/9247], Loss: 3.1582, Perplexity: 23.5270\n",
      "Epoch [1/2], Step [480/9247], Loss: 3.4543, Perplexity: 31.6360\n",
      "Epoch [1/2], Step [500/9247], Loss: 3.0634, Perplexity: 21.4005\n",
      "Epoch [1/2], Step [520/9247], Loss: 3.2940, Perplexity: 26.9509\n",
      "Epoch [1/2], Step [540/9247], Loss: 3.2646, Perplexity: 26.1703\n",
      "Epoch [1/2], Step [560/9247], Loss: 3.1817, Perplexity: 24.0880\n",
      "Epoch [1/2], Step [580/9247], Loss: 3.3230, Perplexity: 27.7439\n",
      "Epoch [1/2], Step [600/9247], Loss: 3.5809, Perplexity: 35.9047\n",
      "Epoch [1/2], Step [620/9247], Loss: 3.2821, Perplexity: 26.6316\n",
      "Epoch [1/2], Step [640/9247], Loss: 3.4213, Perplexity: 30.6094\n",
      "Epoch [1/2], Step [660/9247], Loss: 2.7771, Perplexity: 16.0725\n",
      "Epoch [1/2], Step [680/9247], Loss: 2.9892, Perplexity: 19.8695\n",
      "Epoch [1/2], Step [700/9247], Loss: 2.9803, Perplexity: 19.6929\n",
      "Epoch [1/2], Step [720/9247], Loss: 3.5184, Perplexity: 33.7291\n",
      "Epoch [1/2], Step [740/9247], Loss: 2.9205, Perplexity: 18.5506\n",
      "Epoch [1/2], Step [760/9247], Loss: 3.0782, Perplexity: 21.7203\n",
      "Epoch [1/2], Step [780/9247], Loss: 3.1503, Perplexity: 23.3437\n",
      "Epoch [1/2], Step [800/9247], Loss: 2.9981, Perplexity: 20.0471\n",
      "Epoch [1/2], Step [820/9247], Loss: 2.7934, Perplexity: 16.3365\n",
      "Epoch [1/2], Step [840/9247], Loss: 2.9675, Perplexity: 19.4438\n",
      "Epoch [1/2], Step [860/9247], Loss: 2.8653, Perplexity: 17.5541\n",
      "Epoch [1/2], Step [880/9247], Loss: 2.9721, Perplexity: 19.5320\n",
      "Epoch [1/2], Step [900/9247], Loss: 2.9288, Perplexity: 18.7050\n",
      "Epoch [1/2], Step [920/9247], Loss: 2.7490, Perplexity: 15.6266\n",
      "Epoch [1/2], Step [940/9247], Loss: 2.7991, Perplexity: 16.4292\n",
      "Epoch [1/2], Step [960/9247], Loss: 2.8135, Perplexity: 16.6676\n",
      "Epoch [1/2], Step [980/9247], Loss: 3.0805, Perplexity: 21.7690\n",
      "Epoch [1/2], Step [1000/9247], Loss: 2.9113, Perplexity: 18.3807\n",
      "Epoch [1/2], Step [1020/9247], Loss: 2.8871, Perplexity: 17.9418\n",
      "Epoch [1/2], Step [1040/9247], Loss: 3.0532, Perplexity: 21.1840\n",
      "Epoch [1/2], Step [1060/9247], Loss: 2.5768, Perplexity: 13.1543\n",
      "Epoch [1/2], Step [1080/9247], Loss: 2.9734, Perplexity: 19.5583\n",
      "Epoch [1/2], Step [1100/9247], Loss: 2.6421, Perplexity: 14.0430\n",
      "Epoch [1/2], Step [1120/9247], Loss: 2.8203, Perplexity: 16.7823\n",
      "Epoch [1/2], Step [1140/9247], Loss: 2.7615, Perplexity: 15.8242\n",
      "Epoch [1/2], Step [1160/9247], Loss: 2.8725, Perplexity: 17.6813\n",
      "Epoch [1/2], Step [1180/9247], Loss: 2.6780, Perplexity: 14.5564\n",
      "Epoch [1/2], Step [1200/9247], Loss: 2.8674, Perplexity: 17.5913\n",
      "Epoch [1/2], Step [1220/9247], Loss: 3.2522, Perplexity: 25.8483\n",
      "Epoch [1/2], Step [1240/9247], Loss: 2.9593, Perplexity: 19.2852\n",
      "Epoch [1/2], Step [1260/9247], Loss: 2.6333, Perplexity: 13.9203\n",
      "Epoch [1/2], Step [1280/9247], Loss: 2.5945, Perplexity: 13.3897\n",
      "Epoch [1/2], Step [1300/9247], Loss: 3.0186, Perplexity: 20.4628\n",
      "Epoch [1/2], Step [1320/9247], Loss: 2.6320, Perplexity: 13.9014\n",
      "Epoch [1/2], Step [1340/9247], Loss: 2.5588, Perplexity: 12.9207\n",
      "Epoch [1/2], Step [1360/9247], Loss: 2.6857, Perplexity: 14.6682\n",
      "Epoch [1/2], Step [1380/9247], Loss: 2.5498, Perplexity: 12.8051\n",
      "Epoch [1/2], Step [1400/9247], Loss: 2.6707, Perplexity: 14.4500\n",
      "Epoch [1/2], Step [1420/9247], Loss: 3.1941, Perplexity: 24.3887\n",
      "Epoch [1/2], Step [1440/9247], Loss: 2.7264, Perplexity: 15.2778\n",
      "Epoch [1/2], Step [1460/9247], Loss: 2.6532, Perplexity: 14.1992\n",
      "Epoch [1/2], Step [1480/9247], Loss: 2.8205, Perplexity: 16.7856\n",
      "Epoch [1/2], Step [1500/9247], Loss: 2.5906, Perplexity: 13.3377\n",
      "Epoch [1/2], Step [1520/9247], Loss: 2.6323, Perplexity: 13.9062\n",
      "Epoch [1/2], Step [1540/9247], Loss: 3.5230, Perplexity: 33.8859\n",
      "Epoch [1/2], Step [1560/9247], Loss: 2.5654, Perplexity: 13.0060\n",
      "Epoch [1/2], Step [1580/9247], Loss: 2.4099, Perplexity: 11.1333\n",
      "Epoch [1/2], Step [1600/9247], Loss: 2.5385, Perplexity: 12.6601\n",
      "Epoch [1/2], Step [1620/9247], Loss: 2.7768, Perplexity: 16.0682\n",
      "Epoch [1/2], Step [1640/9247], Loss: 3.3457, Perplexity: 28.3808\n",
      "Epoch [1/2], Step [1660/9247], Loss: 2.9075, Perplexity: 18.3106\n",
      "Epoch [1/2], Step [1680/9247], Loss: 2.4668, Perplexity: 11.7843\n",
      "Epoch [1/2], Step [1700/9247], Loss: 2.5460, Perplexity: 12.7556\n",
      "Epoch [1/2], Step [1720/9247], Loss: 2.8399, Perplexity: 17.1148\n",
      "Epoch [1/2], Step [1740/9247], Loss: 2.3597, Perplexity: 10.5875\n",
      "Epoch [1/2], Step [1760/9247], Loss: 2.5146, Perplexity: 12.3620\n",
      "Epoch [1/2], Step [1780/9247], Loss: 2.5165, Perplexity: 12.3853\n",
      "Epoch [1/2], Step [1800/9247], Loss: 2.7193, Perplexity: 15.1695\n",
      "Epoch [1/2], Step [1820/9247], Loss: 2.4331, Perplexity: 11.3939\n",
      "Epoch [1/2], Step [1840/9247], Loss: 2.9508, Perplexity: 19.1214\n",
      "Epoch [1/2], Step [1860/9247], Loss: 2.3947, Perplexity: 10.9644\n",
      "Epoch [1/2], Step [1880/9247], Loss: 2.7821, Perplexity: 16.1527\n",
      "Epoch [1/2], Step [1900/9247], Loss: 2.6552, Perplexity: 14.2273\n",
      "Epoch [1/2], Step [1920/9247], Loss: 2.6901, Perplexity: 14.7338\n",
      "Epoch [1/2], Step [1940/9247], Loss: 2.6145, Perplexity: 13.6602\n",
      "Epoch [1/2], Step [1960/9247], Loss: 2.2864, Perplexity: 9.8390\n",
      "Epoch [1/2], Step [1980/9247], Loss: 2.3832, Perplexity: 10.8398\n",
      "Epoch [1/2], Step [2000/9247], Loss: 2.7827, Perplexity: 16.1627\n",
      "Epoch [1/2], Step [2020/9247], Loss: 2.3287, Perplexity: 10.2641\n",
      "Epoch [1/2], Step [2040/9247], Loss: 2.4095, Perplexity: 11.1283\n",
      "Epoch [1/2], Step [2060/9247], Loss: 2.5424, Perplexity: 12.7105\n",
      "Epoch [1/2], Step [2080/9247], Loss: 2.4590, Perplexity: 11.6927\n",
      "Epoch [1/2], Step [2100/9247], Loss: 2.4197, Perplexity: 11.2423\n",
      "Epoch [1/2], Step [2120/9247], Loss: 2.2539, Perplexity: 9.5246\n",
      "Epoch [1/2], Step [2140/9247], Loss: 2.5344, Perplexity: 12.6086\n",
      "Epoch [1/2], Step [2160/9247], Loss: 2.5416, Perplexity: 12.6994\n",
      "Epoch [1/2], Step [2180/9247], Loss: 2.4596, Perplexity: 11.7000\n",
      "Epoch [1/2], Step [2200/9247], Loss: 2.3495, Perplexity: 10.4800\n",
      "Epoch [1/2], Step [2220/9247], Loss: 2.5096, Perplexity: 12.3004\n",
      "Epoch [1/2], Step [2240/9247], Loss: 2.5997, Perplexity: 13.4601\n",
      "Epoch [1/2], Step [2260/9247], Loss: 2.4119, Perplexity: 11.1555\n",
      "Epoch [1/2], Step [2280/9247], Loss: 2.9797, Perplexity: 19.6820\n",
      "Epoch [1/2], Step [2300/9247], Loss: 2.4872, Perplexity: 12.0270\n",
      "Epoch [1/2], Step [2320/9247], Loss: 2.2900, Perplexity: 9.8750\n",
      "Epoch [1/2], Step [2340/9247], Loss: 2.3635, Perplexity: 10.6285\n",
      "Epoch [1/2], Step [2360/9247], Loss: 3.3058, Perplexity: 27.2703\n",
      "Epoch [1/2], Step [2380/9247], Loss: 2.4956, Perplexity: 12.1295\n",
      "Epoch [1/2], Step [2400/9247], Loss: 2.5226, Perplexity: 12.4612\n",
      "Epoch [1/2], Step [2420/9247], Loss: 2.2965, Perplexity: 9.9395\n",
      "Epoch [1/2], Step [2440/9247], Loss: 2.2853, Perplexity: 9.8288\n",
      "Epoch [1/2], Step [2460/9247], Loss: 2.5121, Perplexity: 12.3306\n",
      "Epoch [1/2], Step [2480/9247], Loss: 2.3999, Perplexity: 11.0219\n",
      "Epoch [1/2], Step [2500/9247], Loss: 2.1788, Perplexity: 8.8356\n",
      "Epoch [1/2], Step [2520/9247], Loss: 2.3439, Perplexity: 10.4218\n",
      "Epoch [1/2], Step [2540/9247], Loss: 2.5636, Perplexity: 12.9828\n",
      "Epoch [1/2], Step [2560/9247], Loss: 2.4325, Perplexity: 11.3878\n",
      "Epoch [1/2], Step [2580/9247], Loss: 2.2170, Perplexity: 9.1794\n",
      "Epoch [1/2], Step [2600/9247], Loss: 2.5330, Perplexity: 12.5915\n",
      "Epoch [1/2], Step [2620/9247], Loss: 2.2358, Perplexity: 9.3540\n",
      "Epoch [1/2], Step [2640/9247], Loss: 2.1755, Perplexity: 8.8064\n",
      "Epoch [1/2], Step [2660/9247], Loss: 2.2681, Perplexity: 9.6615\n",
      "Epoch [1/2], Step [2680/9247], Loss: 2.5034, Perplexity: 12.2239\n",
      "Epoch [1/2], Step [2700/9247], Loss: 2.4619, Perplexity: 11.7272\n",
      "Epoch [1/2], Step [2720/9247], Loss: 2.4234, Perplexity: 11.2841\n",
      "Epoch [1/2], Step [2740/9247], Loss: 3.1573, Perplexity: 23.5061\n",
      "Epoch [1/2], Step [2760/9247], Loss: 2.3776, Perplexity: 10.7795\n",
      "Epoch [1/2], Step [2780/9247], Loss: 2.3168, Perplexity: 10.1428\n",
      "Epoch [1/2], Step [2800/9247], Loss: 2.4759, Perplexity: 11.8929\n",
      "Epoch [1/2], Step [2820/9247], Loss: 2.3919, Perplexity: 10.9338\n",
      "Epoch [1/2], Step [2840/9247], Loss: 2.1804, Perplexity: 8.8495\n",
      "Epoch [1/2], Step [2860/9247], Loss: 2.3962, Perplexity: 10.9809\n",
      "Epoch [1/2], Step [2880/9247], Loss: 2.6352, Perplexity: 13.9462\n",
      "Epoch [1/2], Step [2900/9247], Loss: 2.4631, Perplexity: 11.7414\n",
      "Epoch [1/2], Step [2920/9247], Loss: 2.2435, Perplexity: 9.4261\n",
      "Epoch [1/2], Step [2940/9247], Loss: 2.4013, Perplexity: 11.0378\n",
      "Epoch [1/2], Step [2960/9247], Loss: 2.3561, Perplexity: 10.5497\n",
      "Epoch [1/2], Step [2980/9247], Loss: 2.5033, Perplexity: 12.2230\n",
      "Epoch [1/2], Step [3000/9247], Loss: 2.3346, Perplexity: 10.3258\n",
      "Epoch [1/2], Step [3020/9247], Loss: 2.7826, Perplexity: 16.1614\n",
      "Epoch [1/2], Step [3040/9247], Loss: 2.3272, Perplexity: 10.2492\n",
      "Epoch [1/2], Step [3060/9247], Loss: 2.1734, Perplexity: 8.7884\n",
      "Epoch [1/2], Step [3080/9247], Loss: 3.2636, Perplexity: 26.1430\n",
      "Epoch [1/2], Step [3100/9247], Loss: 2.2533, Perplexity: 9.5194\n",
      "Epoch [1/2], Step [3120/9247], Loss: 2.4587, Perplexity: 11.6894\n",
      "Epoch [1/2], Step [3140/9247], Loss: 2.3682, Perplexity: 10.6777\n",
      "Epoch [1/2], Step [3160/9247], Loss: 2.1002, Perplexity: 8.1675\n",
      "Epoch [1/2], Step [3180/9247], Loss: 2.8317, Perplexity: 16.9746\n",
      "Epoch [1/2], Step [3200/9247], Loss: 2.7609, Perplexity: 15.8144\n",
      "Epoch [1/2], Step [3220/9247], Loss: 2.4494, Perplexity: 11.5816\n",
      "Epoch [1/2], Step [3240/9247], Loss: 2.2755, Perplexity: 9.7328\n",
      "Epoch [1/2], Step [3260/9247], Loss: 2.6242, Perplexity: 13.7936\n",
      "Epoch [1/2], Step [3280/9247], Loss: 2.4151, Perplexity: 11.1911\n",
      "Epoch [1/2], Step [3300/9247], Loss: 2.6149, Perplexity: 13.6658\n",
      "Epoch [1/2], Step [3320/9247], Loss: 2.9466, Perplexity: 19.0418\n",
      "Epoch [1/2], Step [3340/9247], Loss: 3.2032, Perplexity: 24.6120\n",
      "Epoch [1/2], Step [3360/9247], Loss: 2.3812, Perplexity: 10.8175\n",
      "Epoch [1/2], Step [3380/9247], Loss: 2.6623, Perplexity: 14.3293\n",
      "Epoch [1/2], Step [3400/9247], Loss: 2.3743, Perplexity: 10.7438\n",
      "Epoch [1/2], Step [3420/9247], Loss: 2.4251, Perplexity: 11.3030\n",
      "Epoch [1/2], Step [3440/9247], Loss: 2.4645, Perplexity: 11.7579\n",
      "Epoch [1/2], Step [3460/9247], Loss: 2.5129, Perplexity: 12.3410\n",
      "Epoch [1/2], Step [3480/9247], Loss: 2.5625, Perplexity: 12.9676\n",
      "Epoch [1/2], Step [3500/9247], Loss: 2.1902, Perplexity: 8.9373\n",
      "Epoch [1/2], Step [3520/9247], Loss: 2.2503, Perplexity: 9.4904\n",
      "Epoch [1/2], Step [3540/9247], Loss: 2.3926, Perplexity: 10.9418\n",
      "Epoch [1/2], Step [3560/9247], Loss: 2.3953, Perplexity: 10.9717\n",
      "Epoch [1/2], Step [3580/9247], Loss: 2.3001, Perplexity: 9.9754\n",
      "Epoch [1/2], Step [3600/9247], Loss: 2.7921, Perplexity: 16.3160\n",
      "Epoch [1/2], Step [3620/9247], Loss: 2.3838, Perplexity: 10.8465\n",
      "Epoch [1/2], Step [3640/9247], Loss: 2.3119, Perplexity: 10.0937\n",
      "Epoch [1/2], Step [3660/9247], Loss: 2.1505, Perplexity: 8.5893\n",
      "Epoch [1/2], Step [3680/9247], Loss: 2.6448, Perplexity: 14.0803\n",
      "Epoch [1/2], Step [3700/9247], Loss: 2.4933, Perplexity: 12.1011\n",
      "Epoch [1/2], Step [3720/9247], Loss: 2.3412, Perplexity: 10.3939\n",
      "Epoch [1/2], Step [3740/9247], Loss: 2.2383, Perplexity: 9.3778\n",
      "Epoch [1/2], Step [3760/9247], Loss: 2.1930, Perplexity: 8.9625\n",
      "Epoch [1/2], Step [3780/9247], Loss: 2.7549, Perplexity: 15.7201\n",
      "Epoch [1/2], Step [3800/9247], Loss: 2.3359, Perplexity: 10.3391\n",
      "Epoch [1/2], Step [3820/9247], Loss: 2.1070, Perplexity: 8.2232\n",
      "Epoch [1/2], Step [3840/9247], Loss: 2.1342, Perplexity: 8.4506\n",
      "Epoch [1/2], Step [3860/9247], Loss: 2.0923, Perplexity: 8.1036\n",
      "Epoch [1/2], Step [3880/9247], Loss: 2.1163, Perplexity: 8.3007\n",
      "Epoch [1/2], Step [3900/9247], Loss: 2.5234, Perplexity: 12.4710\n",
      "Epoch [1/2], Step [3920/9247], Loss: 2.2465, Perplexity: 9.4548\n",
      "Epoch [1/2], Step [3940/9247], Loss: 2.2214, Perplexity: 9.2203\n",
      "Epoch [1/2], Step [3960/9247], Loss: 2.2918, Perplexity: 9.8927\n",
      "Epoch [1/2], Step [3980/9247], Loss: 3.1112, Perplexity: 22.4480\n",
      "Epoch [1/2], Step [4000/9247], Loss: 2.2944, Perplexity: 9.9187\n",
      "Epoch [1/2], Step [4020/9247], Loss: 2.5236, Perplexity: 12.4740\n",
      "Epoch [1/2], Step [4040/9247], Loss: 2.6363, Perplexity: 13.9617\n",
      "Epoch [1/2], Step [4060/9247], Loss: 2.4389, Perplexity: 11.4607\n",
      "Epoch [1/2], Step [4080/9247], Loss: 2.2145, Perplexity: 9.1564\n",
      "Epoch [1/2], Step [4100/9247], Loss: 2.1787, Perplexity: 8.8350\n",
      "Epoch [1/2], Step [4120/9247], Loss: 2.2112, Perplexity: 9.1267\n",
      "Epoch [1/2], Step [4140/9247], Loss: 2.3824, Perplexity: 10.8312\n",
      "Epoch [1/2], Step [4160/9247], Loss: 2.3937, Perplexity: 10.9543\n",
      "Epoch [1/2], Step [4180/9247], Loss: 2.3591, Perplexity: 10.5810\n",
      "Epoch [1/2], Step [4200/9247], Loss: 2.0857, Perplexity: 8.0501\n",
      "Epoch [1/2], Step [4220/9247], Loss: 2.3363, Perplexity: 10.3429\n",
      "Epoch [1/2], Step [4240/9247], Loss: 2.3891, Perplexity: 10.9039\n",
      "Epoch [1/2], Step [4260/9247], Loss: 2.1112, Perplexity: 8.2578\n",
      "Epoch [1/2], Step [4280/9247], Loss: 2.2167, Perplexity: 9.1771\n",
      "Epoch [1/2], Step [4300/9247], Loss: 2.3160, Perplexity: 10.1348\n",
      "Epoch [1/2], Step [4320/9247], Loss: 2.4274, Perplexity: 11.3297\n",
      "Epoch [1/2], Step [4340/9247], Loss: 2.5499, Perplexity: 12.8052\n",
      "Epoch [1/2], Step [4360/9247], Loss: 2.3816, Perplexity: 10.8227\n",
      "Epoch [1/2], Step [4380/9247], Loss: 2.1338, Perplexity: 8.4471\n",
      "Epoch [1/2], Step [4400/9247], Loss: 2.1654, Perplexity: 8.7178\n",
      "Epoch [1/2], Step [4420/9247], Loss: 2.1355, Perplexity: 8.4615\n",
      "Epoch [1/2], Step [4440/9247], Loss: 2.4911, Perplexity: 12.0751\n",
      "Epoch [1/2], Step [4460/9247], Loss: 2.3392, Perplexity: 10.3733\n",
      "Epoch [1/2], Step [4480/9247], Loss: 2.1703, Perplexity: 8.7613\n",
      "Epoch [1/2], Step [4500/9247], Loss: 2.4897, Perplexity: 12.0573\n",
      "Epoch [1/2], Step [4520/9247], Loss: 2.1941, Perplexity: 8.9724\n",
      "Epoch [1/2], Step [4540/9247], Loss: 2.3795, Perplexity: 10.7990\n",
      "Epoch [1/2], Step [4560/9247], Loss: 2.4105, Perplexity: 11.1396\n",
      "Epoch [1/2], Step [4580/9247], Loss: 2.1252, Perplexity: 8.3742\n",
      "Epoch [1/2], Step [4600/9247], Loss: 2.0732, Perplexity: 7.9502\n",
      "Epoch [1/2], Step [4620/9247], Loss: 2.1682, Perplexity: 8.7421\n",
      "Epoch [1/2], Step [4640/9247], Loss: 2.1742, Perplexity: 8.7951\n",
      "Epoch [1/2], Step [4660/9247], Loss: 2.1443, Perplexity: 8.5362\n",
      "Epoch [1/2], Step [4680/9247], Loss: 2.1996, Perplexity: 9.0216\n",
      "Epoch [1/2], Step [4700/9247], Loss: 2.3461, Perplexity: 10.4445\n",
      "Epoch [1/2], Step [4720/9247], Loss: 2.3576, Perplexity: 10.5652\n",
      "Epoch [1/2], Step [4740/9247], Loss: 2.3883, Perplexity: 10.8947\n",
      "Epoch [1/2], Step [4760/9247], Loss: 2.2808, Perplexity: 9.7844\n",
      "Epoch [1/2], Step [4780/9247], Loss: 2.3468, Perplexity: 10.4516\n",
      "Epoch [1/2], Step [4800/9247], Loss: 2.1667, Perplexity: 8.7293\n",
      "Epoch [1/2], Step [4820/9247], Loss: 2.1331, Perplexity: 8.4407\n",
      "Epoch [1/2], Step [4840/9247], Loss: 2.2045, Perplexity: 9.0661\n",
      "Epoch [1/2], Step [4860/9247], Loss: 2.0547, Perplexity: 7.8047\n",
      "Epoch [1/2], Step [4880/9247], Loss: 2.2123, Perplexity: 9.1365\n",
      "Epoch [1/2], Step [4900/9247], Loss: 2.1100, Perplexity: 8.2479\n",
      "Epoch [1/2], Step [4920/9247], Loss: 2.3285, Perplexity: 10.2624\n",
      "Epoch [1/2], Step [4940/9247], Loss: 2.1685, Perplexity: 8.7452\n",
      "Epoch [1/2], Step [4960/9247], Loss: 2.9371, Perplexity: 18.8602\n",
      "Epoch [1/2], Step [4980/9247], Loss: 2.8539, Perplexity: 17.3554\n",
      "Epoch [1/2], Step [5000/9247], Loss: 2.3393, Perplexity: 10.3737\n",
      "Epoch [1/2], Step [5020/9247], Loss: 2.1867, Perplexity: 8.9054\n",
      "Epoch [1/2], Step [5040/9247], Loss: 2.2684, Perplexity: 9.6642\n",
      "Epoch [1/2], Step [5060/9247], Loss: 2.1912, Perplexity: 8.9456\n",
      "Epoch [1/2], Step [5080/9247], Loss: 2.1291, Perplexity: 8.4076\n",
      "Epoch [1/2], Step [5100/9247], Loss: 2.1360, Perplexity: 8.4653\n",
      "Epoch [1/2], Step [5120/9247], Loss: 2.0890, Perplexity: 8.0766\n",
      "Epoch [1/2], Step [5140/9247], Loss: 2.3781, Perplexity: 10.7839\n",
      "Epoch [1/2], Step [5160/9247], Loss: 2.1825, Perplexity: 8.8689\n",
      "Epoch [1/2], Step [5180/9247], Loss: 2.3334, Perplexity: 10.3127\n",
      "Epoch [1/2], Step [5200/9247], Loss: 2.5205, Perplexity: 12.4348\n",
      "Epoch [1/2], Step [5220/9247], Loss: 2.8646, Perplexity: 17.5418\n",
      "Epoch [1/2], Step [5240/9247], Loss: 2.2164, Perplexity: 9.1740\n",
      "Epoch [1/2], Step [5260/9247], Loss: 2.2193, Perplexity: 9.2007\n",
      "Epoch [1/2], Step [5280/9247], Loss: 2.0993, Perplexity: 8.1608\n",
      "Epoch [1/2], Step [5300/9247], Loss: 2.2523, Perplexity: 9.5097\n",
      "Epoch [1/2], Step [5320/9247], Loss: 2.5678, Perplexity: 13.0376\n",
      "Epoch [1/2], Step [5340/9247], Loss: 2.2248, Perplexity: 9.2514\n",
      "Epoch [1/2], Step [5360/9247], Loss: 2.0525, Perplexity: 7.7872\n",
      "Epoch [1/2], Step [5380/9247], Loss: 2.2445, Perplexity: 9.4355\n",
      "Epoch [1/2], Step [5400/9247], Loss: 2.1344, Perplexity: 8.4522\n",
      "Epoch [1/2], Step [5420/9247], Loss: 2.3840, Perplexity: 10.8478\n",
      "Epoch [1/2], Step [5440/9247], Loss: 2.3624, Perplexity: 10.6161\n",
      "Epoch [1/2], Step [5460/9247], Loss: 2.1404, Perplexity: 8.5025\n",
      "Epoch [1/2], Step [5480/9247], Loss: 1.9595, Perplexity: 7.0955\n",
      "Epoch [1/2], Step [5500/9247], Loss: 2.1832, Perplexity: 8.8744\n",
      "Epoch [1/2], Step [5520/9247], Loss: 2.6084, Perplexity: 13.5775\n",
      "Epoch [1/2], Step [5540/9247], Loss: 2.1611, Perplexity: 8.6803\n",
      "Epoch [1/2], Step [5560/9247], Loss: 2.5094, Perplexity: 12.2977\n",
      "Epoch [1/2], Step [5580/9247], Loss: 2.2844, Perplexity: 9.8197\n",
      "Epoch [1/2], Step [5600/9247], Loss: 2.5329, Perplexity: 12.5900\n",
      "Epoch [1/2], Step [5620/9247], Loss: 2.2492, Perplexity: 9.4806\n",
      "Epoch [1/2], Step [5640/9247], Loss: 2.1711, Perplexity: 8.7680\n",
      "Epoch [1/2], Step [5660/9247], Loss: 2.3012, Perplexity: 9.9859\n",
      "Epoch [1/2], Step [5680/9247], Loss: 1.9517, Perplexity: 7.0408\n",
      "Epoch [1/2], Step [5700/9247], Loss: 2.6414, Perplexity: 14.0322\n",
      "Epoch [1/2], Step [5720/9247], Loss: 2.0342, Perplexity: 7.6463\n",
      "Epoch [1/2], Step [5740/9247], Loss: 2.2476, Perplexity: 9.4652\n",
      "Epoch [1/2], Step [5760/9247], Loss: 2.4269, Perplexity: 11.3240\n",
      "Epoch [1/2], Step [5780/9247], Loss: 2.1677, Perplexity: 8.7381\n",
      "Epoch [1/2], Step [5800/9247], Loss: 2.1703, Perplexity: 8.7611\n",
      "Epoch [1/2], Step [5820/9247], Loss: 2.2154, Perplexity: 9.1647\n",
      "Epoch [1/2], Step [5840/9247], Loss: 2.1107, Perplexity: 8.2537\n",
      "Epoch [1/2], Step [5860/9247], Loss: 2.0647, Perplexity: 7.8829\n",
      "Epoch [1/2], Step [5880/9247], Loss: 2.2713, Perplexity: 9.6917\n",
      "Epoch [1/2], Step [5900/9247], Loss: 3.0489, Perplexity: 21.0922\n",
      "Epoch [1/2], Step [5920/9247], Loss: 2.1843, Perplexity: 8.8845\n",
      "Epoch [1/2], Step [5940/9247], Loss: 2.1023, Perplexity: 8.1852\n",
      "Epoch [1/2], Step [5960/9247], Loss: 2.3086, Perplexity: 10.0602\n",
      "Epoch [1/2], Step [5980/9247], Loss: 2.3186, Perplexity: 10.1611\n",
      "Epoch [1/2], Step [6000/9247], Loss: 1.9688, Perplexity: 7.1618\n",
      "Epoch [1/2], Step [6020/9247], Loss: 2.1960, Perplexity: 8.9890\n",
      "Epoch [1/2], Step [6040/9247], Loss: 2.2082, Perplexity: 9.0996\n",
      "Epoch [1/2], Step [6060/9247], Loss: 2.1110, Perplexity: 8.2565\n",
      "Epoch [1/2], Step [6080/9247], Loss: 2.1585, Perplexity: 8.6578\n",
      "Epoch [1/2], Step [6100/9247], Loss: 2.2855, Perplexity: 9.8309\n",
      "Epoch [1/2], Step [6120/9247], Loss: 1.9758, Perplexity: 7.2125\n",
      "Epoch [1/2], Step [6140/9247], Loss: 1.9295, Perplexity: 6.8861\n",
      "Epoch [1/2], Step [6160/9247], Loss: 1.9557, Perplexity: 7.0688\n",
      "Epoch [1/2], Step [6180/9247], Loss: 2.3628, Perplexity: 10.6203\n",
      "Epoch [1/2], Step [6200/9247], Loss: 2.1908, Perplexity: 8.9423\n",
      "Epoch [1/2], Step [6220/9247], Loss: 2.4287, Perplexity: 11.3438\n",
      "Epoch [1/2], Step [6240/9247], Loss: 2.0784, Perplexity: 7.9920\n",
      "Epoch [1/2], Step [6260/9247], Loss: 2.3061, Perplexity: 10.0357\n",
      "Epoch [1/2], Step [6280/9247], Loss: 2.1509, Perplexity: 8.5929\n",
      "Epoch [1/2], Step [6300/9247], Loss: 2.2323, Perplexity: 9.3211\n",
      "Epoch [1/2], Step [6320/9247], Loss: 2.1030, Perplexity: 8.1908\n",
      "Epoch [1/2], Step [6340/9247], Loss: 1.9582, Perplexity: 7.0865\n",
      "Epoch [1/2], Step [6360/9247], Loss: 2.1563, Perplexity: 8.6394\n",
      "Epoch [1/2], Step [6380/9247], Loss: 2.2104, Perplexity: 9.1197\n",
      "Epoch [1/2], Step [6400/9247], Loss: 2.1235, Perplexity: 8.3607\n",
      "Epoch [1/2], Step [6420/9247], Loss: 2.0045, Perplexity: 7.4222\n",
      "Epoch [1/2], Step [6440/9247], Loss: 2.2344, Perplexity: 9.3410\n",
      "Epoch [1/2], Step [6460/9247], Loss: 2.1525, Perplexity: 8.6066\n",
      "Epoch [1/2], Step [6480/9247], Loss: 2.0402, Perplexity: 7.6918\n",
      "Epoch [1/2], Step [6500/9247], Loss: 2.1742, Perplexity: 8.7956\n",
      "Epoch [1/2], Step [6520/9247], Loss: 2.1694, Perplexity: 8.7526\n",
      "Epoch [1/2], Step [6540/9247], Loss: 2.0882, Perplexity: 8.0707\n",
      "Epoch [1/2], Step [6560/9247], Loss: 2.2308, Perplexity: 9.3075\n",
      "Epoch [1/2], Step [6580/9247], Loss: 2.2686, Perplexity: 9.6660\n",
      "Epoch [1/2], Step [6600/9247], Loss: 2.1868, Perplexity: 8.9067\n",
      "Epoch [1/2], Step [6620/9247], Loss: 2.3867, Perplexity: 10.8778\n",
      "Epoch [1/2], Step [6640/9247], Loss: 2.2177, Perplexity: 9.1864\n",
      "Epoch [1/2], Step [6660/9247], Loss: 2.0418, Perplexity: 7.7047\n",
      "Epoch [1/2], Step [6680/9247], Loss: 2.0717, Perplexity: 7.9379\n",
      "Epoch [1/2], Step [6700/9247], Loss: 2.0986, Perplexity: 8.1547\n",
      "Epoch [1/2], Step [6720/9247], Loss: 2.4391, Perplexity: 11.4626\n",
      "Epoch [1/2], Step [6740/9247], Loss: 2.0335, Perplexity: 7.6411\n",
      "Epoch [1/2], Step [6760/9247], Loss: 2.1993, Perplexity: 9.0189\n",
      "Epoch [1/2], Step [6780/9247], Loss: 2.2490, Perplexity: 9.4785\n",
      "Epoch [1/2], Step [6800/9247], Loss: 2.1544, Perplexity: 8.6229\n",
      "Epoch [1/2], Step [6820/9247], Loss: 1.9770, Perplexity: 7.2207\n",
      "Epoch [1/2], Step [6840/9247], Loss: 2.2553, Perplexity: 9.5381\n",
      "Epoch [1/2], Step [6860/9247], Loss: 2.1682, Perplexity: 8.7428\n",
      "Epoch [1/2], Step [6880/9247], Loss: 2.2306, Perplexity: 9.3054\n",
      "Epoch [1/2], Step [6900/9247], Loss: 2.1207, Perplexity: 8.3373\n",
      "Epoch [1/2], Step [6920/9247], Loss: 2.4395, Perplexity: 11.4672\n",
      "Epoch [1/2], Step [6940/9247], Loss: 2.1839, Perplexity: 8.8808\n",
      "Epoch [1/2], Step [6960/9247], Loss: 2.1800, Perplexity: 8.8465\n",
      "Epoch [1/2], Step [6980/9247], Loss: 2.2288, Perplexity: 9.2891\n",
      "Epoch [1/2], Step [7000/9247], Loss: 2.0580, Perplexity: 7.8304\n",
      "Epoch [1/2], Step [7020/9247], Loss: 2.1279, Perplexity: 8.3970\n",
      "Epoch [1/2], Step [7040/9247], Loss: 2.0118, Perplexity: 7.4769\n",
      "Epoch [1/2], Step [7060/9247], Loss: 2.0596, Perplexity: 7.8426\n",
      "Epoch [1/2], Step [7080/9247], Loss: 2.0149, Perplexity: 7.4999\n",
      "Epoch [1/2], Step [7100/9247], Loss: 2.1254, Perplexity: 8.3761\n",
      "Epoch [1/2], Step [7120/9247], Loss: 2.3688, Perplexity: 10.6844\n",
      "Epoch [1/2], Step [7140/9247], Loss: 2.2710, Perplexity: 9.6895\n",
      "Epoch [1/2], Step [7160/9247], Loss: 2.1778, Perplexity: 8.8268\n",
      "Epoch [1/2], Step [7180/9247], Loss: 2.1880, Perplexity: 8.9176\n",
      "Epoch [1/2], Step [7200/9247], Loss: 2.1079, Perplexity: 8.2312\n",
      "Epoch [1/2], Step [7220/9247], Loss: 2.6830, Perplexity: 14.6285\n",
      "Epoch [1/2], Step [7240/9247], Loss: 2.3953, Perplexity: 10.9719\n",
      "Epoch [1/2], Step [7260/9247], Loss: 2.3882, Perplexity: 10.8940\n",
      "Epoch [1/2], Step [7280/9247], Loss: 2.1684, Perplexity: 8.7447\n",
      "Epoch [1/2], Step [7300/9247], Loss: 2.0378, Perplexity: 7.6735\n",
      "Epoch [1/2], Step [7320/9247], Loss: 2.1334, Perplexity: 8.4435\n",
      "Epoch [1/2], Step [7340/9247], Loss: 2.6278, Perplexity: 13.8438\n",
      "Epoch [1/2], Step [7360/9247], Loss: 2.2761, Perplexity: 9.7389\n",
      "Epoch [1/2], Step [7380/9247], Loss: 2.3742, Perplexity: 10.7423\n",
      "Epoch [1/2], Step [7400/9247], Loss: 2.2995, Perplexity: 9.9690\n",
      "Epoch [1/2], Step [7420/9247], Loss: 2.2980, Perplexity: 9.9540\n",
      "Epoch [1/2], Step [7440/9247], Loss: 2.5898, Perplexity: 13.3269\n",
      "Epoch [1/2], Step [7460/9247], Loss: 2.2523, Perplexity: 9.5100\n",
      "Epoch [1/2], Step [7480/9247], Loss: 2.1934, Perplexity: 8.9656\n",
      "Epoch [1/2], Step [7500/9247], Loss: 2.3277, Perplexity: 10.2539\n",
      "Epoch [1/2], Step [7520/9247], Loss: 2.0489, Perplexity: 7.7590\n",
      "Epoch [1/2], Step [7540/9247], Loss: 2.2450, Perplexity: 9.4401\n",
      "Epoch [1/2], Step [7560/9247], Loss: 2.0835, Perplexity: 8.0321\n",
      "Epoch [1/2], Step [7580/9247], Loss: 2.0028, Perplexity: 7.4095\n",
      "Epoch [1/2], Step [7600/9247], Loss: 2.1195, Perplexity: 8.3268\n",
      "Epoch [1/2], Step [7620/9247], Loss: 2.0075, Perplexity: 7.4444\n",
      "Epoch [1/2], Step [7640/9247], Loss: 2.2725, Perplexity: 9.7034\n",
      "Epoch [1/2], Step [7660/9247], Loss: 2.0629, Perplexity: 7.8691\n",
      "Epoch [1/2], Step [7680/9247], Loss: 2.0099, Perplexity: 7.4623\n",
      "Epoch [1/2], Step [7700/9247], Loss: 2.0252, Perplexity: 7.5780\n",
      "Epoch [1/2], Step [7720/9247], Loss: 2.2507, Perplexity: 9.4946\n",
      "Epoch [1/2], Step [7740/9247], Loss: 2.0600, Perplexity: 7.8462\n",
      "Epoch [1/2], Step [7760/9247], Loss: 2.1525, Perplexity: 8.6059\n",
      "Epoch [1/2], Step [7780/9247], Loss: 2.1858, Perplexity: 8.8973\n",
      "Epoch [1/2], Step [7800/9247], Loss: 2.5649, Perplexity: 12.9998\n",
      "Epoch [1/2], Step [7820/9247], Loss: 2.2088, Perplexity: 9.1050\n",
      "Epoch [1/2], Step [7840/9247], Loss: 2.2842, Perplexity: 9.8180\n",
      "Epoch [1/2], Step [7860/9247], Loss: 2.1534, Perplexity: 8.6140\n",
      "Epoch [1/2], Step [7880/9247], Loss: 2.3289, Perplexity: 10.2667\n",
      "Epoch [1/2], Step [7900/9247], Loss: 2.1129, Perplexity: 8.2719\n",
      "Epoch [1/2], Step [7920/9247], Loss: 2.2988, Perplexity: 9.9627\n",
      "Epoch [1/2], Step [7940/9247], Loss: 3.0836, Perplexity: 21.8367\n",
      "Epoch [1/2], Step [7960/9247], Loss: 2.2256, Perplexity: 9.2586\n",
      "Epoch [1/2], Step [7980/9247], Loss: 2.3055, Perplexity: 10.0287\n",
      "Epoch [1/2], Step [8000/9247], Loss: 2.2257, Perplexity: 9.2604\n",
      "Epoch [1/2], Step [8020/9247], Loss: 3.7160, Perplexity: 41.1013\n",
      "Epoch [1/2], Step [8040/9247], Loss: 1.9328, Perplexity: 6.9088\n",
      "Epoch [1/2], Step [8060/9247], Loss: 2.1342, Perplexity: 8.4503\n",
      "Epoch [1/2], Step [8080/9247], Loss: 2.1062, Perplexity: 8.2166\n",
      "Epoch [1/2], Step [8100/9247], Loss: 1.9759, Perplexity: 7.2131\n",
      "Epoch [1/2], Step [8120/9247], Loss: 2.0638, Perplexity: 7.8759\n",
      "Epoch [1/2], Step [8140/9247], Loss: 2.1404, Perplexity: 8.5025\n",
      "Epoch [1/2], Step [8160/9247], Loss: 2.0206, Perplexity: 7.5432\n",
      "Epoch [1/2], Step [8180/9247], Loss: 2.0308, Perplexity: 7.6205\n",
      "Epoch [1/2], Step [8200/9247], Loss: 1.9647, Perplexity: 7.1329\n",
      "Epoch [1/2], Step [8220/9247], Loss: 2.5419, Perplexity: 12.7035\n",
      "Epoch [1/2], Step [8240/9247], Loss: 2.4769, Perplexity: 11.9047\n",
      "Epoch [1/2], Step [8260/9247], Loss: 2.2058, Perplexity: 9.0772\n",
      "Epoch [1/2], Step [8280/9247], Loss: 2.0934, Perplexity: 8.1126\n",
      "Epoch [1/2], Step [8300/9247], Loss: 1.9547, Perplexity: 7.0620\n",
      "Epoch [1/2], Step [8320/9247], Loss: 1.9981, Perplexity: 7.3752\n",
      "Epoch [1/2], Step [8340/9247], Loss: 2.1509, Perplexity: 8.5927\n",
      "Epoch [1/2], Step [8360/9247], Loss: 2.1230, Perplexity: 8.3562\n",
      "Epoch [1/2], Step [8380/9247], Loss: 2.2180, Perplexity: 9.1887\n",
      "Epoch [1/2], Step [8400/9247], Loss: 2.1780, Perplexity: 8.8286\n",
      "Epoch [1/2], Step [8420/9247], Loss: 2.2317, Perplexity: 9.3161\n",
      "Epoch [1/2], Step [8440/9247], Loss: 2.0151, Perplexity: 7.5013\n",
      "Epoch [1/2], Step [8460/9247], Loss: 1.9473, Perplexity: 7.0100\n",
      "Epoch [1/2], Step [8480/9247], Loss: 2.3977, Perplexity: 10.9979\n",
      "Epoch [1/2], Step [8500/9247], Loss: 2.1613, Perplexity: 8.6825\n",
      "Epoch [1/2], Step [8520/9247], Loss: 2.1721, Perplexity: 8.7764\n",
      "Epoch [1/2], Step [8540/9247], Loss: 2.1664, Perplexity: 8.7264\n",
      "Epoch [1/2], Step [8560/9247], Loss: 2.1462, Perplexity: 8.5523\n",
      "Epoch [1/2], Step [8580/9247], Loss: 2.2085, Perplexity: 9.1024\n",
      "Epoch [1/2], Step [8600/9247], Loss: 2.2708, Perplexity: 9.6868\n",
      "Epoch [1/2], Step [8620/9247], Loss: 2.1772, Perplexity: 8.8220\n",
      "Epoch [1/2], Step [8640/9247], Loss: 2.1107, Perplexity: 8.2538\n",
      "Epoch [1/2], Step [8660/9247], Loss: 2.4923, Perplexity: 12.0892\n",
      "Epoch [1/2], Step [8680/9247], Loss: 2.0271, Perplexity: 7.5922\n",
      "Epoch [1/2], Step [8700/9247], Loss: 1.9653, Perplexity: 7.1371\n",
      "Epoch [1/2], Step [8720/9247], Loss: 2.2434, Perplexity: 9.4255\n",
      "Epoch [1/2], Step [8740/9247], Loss: 2.2366, Perplexity: 9.3617\n",
      "Epoch [1/2], Step [8760/9247], Loss: 2.0211, Perplexity: 7.5467\n",
      "Epoch [1/2], Step [8780/9247], Loss: 2.0937, Perplexity: 8.1147\n",
      "Epoch [1/2], Step [8800/9247], Loss: 2.0225, Perplexity: 7.5572\n",
      "Epoch [1/2], Step [8820/9247], Loss: 1.9944, Perplexity: 7.3481\n",
      "Epoch [1/2], Step [8840/9247], Loss: 2.0864, Perplexity: 8.0555\n",
      "Epoch [1/2], Step [8860/9247], Loss: 2.3619, Perplexity: 10.6112\n",
      "Epoch [1/2], Step [8880/9247], Loss: 2.0830, Perplexity: 8.0286\n",
      "Epoch [1/2], Step [8900/9247], Loss: 2.1566, Perplexity: 8.6418\n",
      "Epoch [1/2], Step [8920/9247], Loss: 2.3189, Perplexity: 10.1644\n",
      "Epoch [1/2], Step [8940/9247], Loss: 2.1476, Perplexity: 8.5643\n",
      "Epoch [1/2], Step [8960/9247], Loss: 2.0751, Perplexity: 7.9650\n",
      "Epoch [1/2], Step [8980/9247], Loss: 2.7716, Perplexity: 15.9850\n",
      "Epoch [1/2], Step [9000/9247], Loss: 2.0353, Perplexity: 7.6543\n",
      "Epoch [1/2], Step [9020/9247], Loss: 2.1415, Perplexity: 8.5120\n",
      "Epoch [1/2], Step [9040/9247], Loss: 1.9824, Perplexity: 7.2602\n",
      "Epoch [1/2], Step [9060/9247], Loss: 2.2995, Perplexity: 9.9695\n",
      "Epoch [1/2], Step [9080/9247], Loss: 2.1556, Perplexity: 8.6333\n",
      "Epoch [1/2], Step [9100/9247], Loss: 1.9419, Perplexity: 6.9721\n",
      "Epoch [1/2], Step [9120/9247], Loss: 2.1531, Perplexity: 8.6113\n",
      "Epoch [1/2], Step [9140/9247], Loss: 2.5601, Perplexity: 12.9377\n",
      "Epoch [1/2], Step [9160/9247], Loss: 2.0781, Perplexity: 7.9889\n",
      "Epoch [1/2], Step [9180/9247], Loss: 2.0785, Perplexity: 7.9924\n",
      "Epoch [1/2], Step [9200/9247], Loss: 2.0856, Perplexity: 8.0491\n",
      "Epoch [1/2], Step [9220/9247], Loss: 2.0195, Perplexity: 7.5345\n",
      "Epoch [1/2], Step [9240/9247], Loss: 2.1517, Perplexity: 8.5995\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File ./models\\decoder-1.pkl cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Save the weights.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m save_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 53\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoder-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(\n\u001b[0;32m     57\u001b[0m             encoder\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m epoch)\n\u001b[0;32m     58\u001b[0m         )\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Close the training log file.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\serialization.py:627\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    624\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    628\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\serialization.py:501\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\serialization.py:472\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 472\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File ./models\\decoder-1.pkl cannot be opened."
     ]
    }
   ],
   "source": [
    "# Open the training log file.\n",
    "f = open(log_file, \"w\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for i_step in range(1, total_step + 1):\n",
    "\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        # Passing the inputs through the CNN-RNN model\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        # Calculating the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "        # Backwarding pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating the parameters in the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Getting training statistics\n",
    "        stats = (\n",
    "            f\"Epoch [{epoch}/{num_epochs}], Step [{i_step}/{total_step}], \"\n",
    "            f\"Loss: {loss.item():.4f}, Perplexity: {np.exp(loss.item()):.4f}\"\n",
    "        )\n",
    "\n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + \"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print(\"\\r\" + stats)\n",
    "\n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(\n",
    "            decoder.state_dict(), os.path.join(r\"C:\\Users\\pradeep dubey\\Desktop\\project\\imgcaption-env\\Image-Captioning-main\\models\", \"decoder-%d.pkl\" % epoch)\n",
    "        )\n",
    "        torch.save(\n",
    "            encoder.state_dict(), os.path.join(r\"C:\\Users\\pradeep dubey\\Desktop\\project\\imgcaption-env\\Image-Captioning-main\\models\", \"encoder-%d.pkl\" % epoch)\n",
    "        )\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Validating the Model using Bleu Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Validating the Model using Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embed): Embedding(11543, 256)\n",
       "  (lstm): LSTM(256, 512, batch_first=True)\n",
       "  (linear): Linear(in_features=512, out_features=11543, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "            (0.229, 0.224, 0.225),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Create the data loader.\n",
    "val_data_loader = val_get_loader(\n",
    "    transform=transform_test, mode=\"valid\", cocoapi_loc=cocoapi_dir\n",
    ")\n",
    "\n",
    "\n",
    "encoder_file = \"encoder-3.pkl\"\n",
    "decoder_file = \"decoder-3.pkl\"\n",
    "\n",
    "# Initialize the encoder and decoder.\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Moving models to GPU if CUDA is available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Loading the trained weights\n",
    "encoder.load_state_dict(torch.load(os.path.join(\"./models\", encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join(\"./models\", decoder_file)))\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d371f1fcdb94e5492978ffa1fe4fea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# infer captions for all images\n",
    "pred_result = defaultdict(list)\n",
    "for img_id, img in tqdm(val_data_loader):\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = encoder(img).unsqueeze(1)\n",
    "        output = decoder.sample(features)\n",
    "    sentence = clean_sentence(output, val_data_loader.dataset.vocab.idx2word)\n",
    "    pred_result[img_id.item()].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    #os.path.join(cocoapi_dir, \"cocoapi\", \"annotations/captions_val2014.json\"), \"r\"\n",
    "    os.path.join(cocoapi_dir, \"annotations/captions_val2017.json\"), \"r\"\n",
    ") as f:\n",
    "    caption = json.load(f)\n",
    "\n",
    "valid_annot = caption[\"annotations\"]\n",
    "valid_result = defaultdict(list)\n",
    "for i in valid_annot:\n",
    "    valid_result[i[\"image_id\"]].append(i[\"caption\"].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a black honda motorcycle parked in front of a garage.',\n",
       "  'a honda motorcycle parked in a grass driveway',\n",
       "  'a black honda motorcycle with a dark burgundy seat.',\n",
       "  'ma motorcycle parked on the gravel in front of a garage',\n",
       "  'a motorcycle with its brake extended standing outside'],\n",
       " ['an office cubicle with four different types of computers.',\n",
       "  'the home office space seems to be very cluttered.',\n",
       "  'an office with desk computer and chair and laptop.',\n",
       "  'office setting with a lot of computer screens.',\n",
       "  'a desk and chair in an office cubicle.'],\n",
       " ['a small closed toilet in a cramped space.',\n",
       "  'a tan toilet and sink combination in a small room.',\n",
       "  'this is an advanced toilet with a sink and control panel.',\n",
       "  'a close-up picture of a toilet with a fountain.',\n",
       "  'off white toilet with a faucet and controls. ']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(valid_result.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' a kitchen with a stove , refrigerator , andsink .'],\n",
       " [' a cake that is sitting on a table .'],\n",
       " [' a room with a desk , a chair , a bookshelf , and a television .']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pred_result.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18904339403329923"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_score(true_sentences=valid_result, predicted_sentences=pred_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad bleu score with only 3 epochs!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
